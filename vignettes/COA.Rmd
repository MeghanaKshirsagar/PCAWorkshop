---
title: "Running Correspondence Analysis in R"
author: "Aedin Culhane"
date: "July 15, 2020"
output:
  rmarkdown::html_document:
    highlight: pygments
    toc: true
    toc_depth: 3
    fig_width: 5
vignette: >
  %\VignetteIndexEntry{COA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding[utf8]{inputenc}

---
```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
# Set up the environment
opts_chunk$set(cache.path='cache/', fig.path='fig/', cache=FALSE, tidy=FALSE, fig.keep='high', echo=TRUE, dpi=100, warnings=FALSE, message=FALSE, comment=NA, warning=FALSE, results='as.is', fig.width = 10, fig.height = 6) 
```

### Necessary R libraries

```{r libraries}
# Install, if necessary, and load necessary libraries and set up R session
#if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager")
library(ggplot2)
library(ggfortify)
library(corral)
library(ade4)
library(irlba)
library(glmpca)
library(ggplot2)
library(scatterD3)
library(ComplexHeatmap)
```


## Background

Decomposition of the Pearson residuals is a excellent method for exploratory analysis of count data and is also know as correspondence analysis. Because of its speed (which is comparable to PCA) and its performance it has been adopted by many fields. Correspondence analysis was originally proposed in 1935 (Hirschfeld, 1935), and was later developed by Benzécri as part of the French duality diagram framework for multivariate statistics (Benzécri, 1973; Holmes, 2008). His trainee Michael Greenacre later popularized its use with large, sparse count data in diverse settings and disciplines, including linguistics, business and marketing research, and archaeology (Greenacre, 1984, 2010). In numerical ecology, it is commonly applied to analyzing species abundance count matrices (M. Greenacre, 2013; Legendre et al., 1998). It has been successfully  applied to microarray transcriptomics data (Fellenberg et al., 2001). and a Bioconductor version was implemented in made4 Bioconductor package by Culhane et al., (2002) and the method was also used in joint and multi-table factorization methods (Culhane et al.,  2003; Meng et al., 2014). Its first application on single cell RNA seq was in analysis of metagenomic scRNAseq microbiome census data data (McMurdie & Holmes, 2013). Correspondence analysis is available in the ade4 and vegan R packages and was listed in a review of single and multi-table matrix factorization approaches (Meng C, Zeleznik OA. et al., 2016).

Correspondence analysis (COA) is considered a dual-scaling method, because both the rows and columns are scaled prior to singular value decomposition (svd). Lauren Hsu provided an description of COA applied to one or an extension to joint decomposition of multi single cell RNA seq in her masters thesis (Biostatistics, Harvard School of Public Health, 2020) and has implemented an fast implementation of COA that supports sparse matrix operations and scalable svd (IRBLA) in her Bioconductor package Corral.   

Hafemeister and Satija in their characterization of the SCTransform method  propose that Pearson residuals of a regularized negative binomial model (a generalized linear model with sequencing depth as a covariate) could be used to remove technical characteristics while preserving biological heterogeneity, with the residuals used directly as input for downstream analysis, in place of log-transformed counts (Hafemeister & Satija, 2019) In the Townes et al., (2019), generalized principal component analysis (GLM-PCA), a generalization of PCA to exponential family likelihoods but report that both the multinomial and Dirichlet-multinomial are computationally intractable on single cell data and propose Poisson and negative binomial likelihood models, but focuses on Poisson glmPCA due to its performance. 

Single cell RNAseq data are counts and naturally Poisson, therefore the application of Pearson Residuals by both Townes et al., (2019) and Hafemeister and Satija (2019) shows how correspondence analysis has been “rediscovered”. In reviews by Abdi & Valentin, 2007; Greenacre, 2010, they remark how frequently COA is rediscovered but each rediscovery enforces the importance of it to the field 

In this vignette we compare different approaches for applying COA in R.

## Toy Dataset 

The function simData() in the glmPCA vignette, orginally provided by [Jake Yeung](https://github.com/willtownes/scrna2019/issues/2) created a simulated dataset with 4989 rows and 150 columns, 3 biological groups (clusters) of 50 cells each and 2 batches. 5000 rows are created but some are filtered to create a matrix of 4989 rows.


```{r}
simData<-function() {
    set.seed(202)
  ngenes <- 5000 #must be divisible by 10
  ngenes_informative<-ngenes*.1
  ncells <- 50 #number of cells per cluster, must be divisible by 2
  nclust<- 3
  # simulate two batches with different depths
  batch<-rep(1:2, each = nclust*ncells/2)
  ncounts <- rpois(ncells*nclust, lambda = 1000*batch)
  # generate profiles for 3 clusters
  profiles_informative <- replicate(nclust, exp(rnorm(ngenes_informative)))
  profiles_const<-matrix(ncol=nclust,rep(exp(rnorm(ngenes-ngenes_informative)),nclust))
  profiles <- rbind(profiles_informative,profiles_const)
  # generate cluster labels
  clust <- sample(rep(1:3, each = ncells))
  # generate single-cell transcriptomes 
  counts <- sapply(seq_along(clust), function(i){
    rmultinom(1, ncounts[i], prob = profiles[,clust[i]])
  })
  rownames(counts) <- paste("gene", seq(nrow(counts)), sep = "_")
  colnames(counts) <- paste("cell", seq(ncol(counts)), sep = "_")
  # clean up rows
  Y <- counts[rowSums(counts) > 0, ]
  sz<-colSums(Y)
  Ycpm<-1e6*t(t(Y)/sz)
  Yl2<-log2(1+Ycpm)
  z<-log10(sz)
  pz<-1-colMeans(Y>0)
  cm<-data.frame(total_counts=sz,zero_frac=pz,clust=factor(clust),batch=factor(batch))
  return(list(Y=Y, pheno=cm))
}
simDataObj <-simData()
mat=simDataObj$Y
pheno=simDataObj$pheno
dim(mat)
```


```{r}
dim(mat)
```

There are 3 clusters and 2 batches
```{r}
table(pheno$clust, pheno$batch)
```

# COA Background




So what does correspondence analysis do and how is it related to decomposotio of the Pearson Residuals. As mentioned above a critical step in decomposition is choosing the form of the data that makes more sense. SVD will detect linear vectors that can the most variance in the data.  So if the variance is skewed or varies with the mean, it needs normalization.  

I will describe it in two ways, you could consider the expected value of an element (Xij) in a matrix N to be the producet of its row (ri) and column (cj) weight, where the row weight is the rowsum/total sum and thus represents the contributes of that row to the total matrix.  Equally the column weight is the colsum/total sum, and is the contribution of that column to the total.  Sometimes the row and column weight are called the row and column mass respectively.  The expected weight of every element is the outer product of the row and column weights 



In COA, the data are treated like a contingency tables so the residuals are the difference between the observed data and the expected under the assumption that there is no relationship.The Pearson chi sq statistics is the observed-expected/ sqft (expected) and this matrix is subject to svd.

```{r coa, echo=FALSE, fig.cap="COA", out.width = '100%'}
knitr::include_graphics("./image/coa.png")
```


A much more detailed explanation of correspondence analysis is provided at https://www.displayr.com/math-correspondence-analysis/

## Expected matrix

The input matrix must be positive count data.

The expected value for each element of the matrix is the product of the row and column weight.  This can be easily calculated

```{r}
rw<-rowSums(mat)/sum(mat) # row weights
cw<-colSums(mat)/sum(mat)  # column weights
expectedValues<- outer(cw, rw)
#dim(expw)
expectedValues[1:2,1:3]

```
Objected values

```{r}
mat[1:3,1:3]
```
Therefore the chi-sq matrix is just as stated above, the difference in the observed - expected / sq(expected)


## PCA of toy data (for comparison)
For comparison we will include a PCA of the 150 cases which form 3 clusters

```{r}

pca1<-prcomp(t(mat), rank. = 2,center=TRUE, scale=TRUE)
summary(pca1)
```


Plot of first two components, colored by cluster.   Autoplot is a wrapper for plotting common R objects, including output from prcomp and princomp. It requires the library ggfortify to run.

```{r, warning=FALSE}
library(ggfortify)
ggplot2::autoplot(pca1, colour=pheno$clust)+
  ggtitle("Clusters")
```
Plot of first two components, colored by batch. PCA does not split the count data by clusters.

```{r, warning=FALSE}
ggplot2::autoplot(pca1, colour=pheno$batch)+
  ggtitle("Batch")
```



## Singular Value Decomposition (svd, irbla)

PCA is computed by an Singular Value Decomposition (svd) of a centered or centered and scaled matrix to compute PCA of the covariance or correlation matrix. SVD can be considered a generalization of eigenvalue decomposition. In eigenvalue decomposition the D matrix is required to be square. 

Given a matrix X  of dimension n×p, SVD decomposes it to:

$X = U D V^{t}$

U,V define the left and right singular values 

U and V are square orthogonal:
$UU^{t} = I_{p}$
$VV^{t} = I_{n}$

the paramter nu,nv defined the number of left and right singular values (components). The output d is a vector of singular values. See the PCA tutorial for more information.  
```{r}
svd1<-svd(scale(t(mat), center = TRUE, scale = TRUE), nu = 2,nv = 2)
lapply(svd1,dim)
```

This is the exact same as a PCA, if we compare the correlation between the left singular values (150 cells)

```{r}
round(cor(svd1$u, pca1$x))
```


## Irbla
Irbla is a fast and memory-efficient way to compute a partial SVD and it often used in scRNAseq application.

When running Irbla, you need to define the number of Lanczos iterations (iter) carried out. 
```{r}
svd2<-irlba::irlba(scale(t(mat), center = TRUE, scale = TRUE), nu = 2,nv = 2)
lapply(svd2,dim)
```
```{r}
round(cor(svd1$u, svd2$u))
```
```{r}
par(mfrow=c(1,2))
plot(svd1$u[,1], svd2$u[,1])
plot(svd1$u[,2], svd2$u[,2])
```

## COA using the ade4 function. dudi.coa
Correspondence analysis is implemented in ade4 (and made4 extends ade4). The implementation of Correspondence analysis in ade4 is slower than corral (see below), so is not recommended for running COA on scRNAseq.  However this is a small dataset, far smaller than a typical scRNAseq data.

The ade4 package provides COA in the function dudi.coa.  In the French school, dudi refers to duality diagram (Benzécri, 1973; Holmes, 2008), which is central to the a geometric school of matrix factorization. 

```{r}

coa1<-ade4::dudi.coa(mat,scannf = FALSE, n=2)
summary(coa1)
```

The resulting row and column scores and coordinates are in the objects li, co, l1, c1, where l here refers to  "lignes" or rows, and c is columns.

```{}
coa1$li  #Row coordinates dims =c(4989,10)
coa1$co  #Col coordinates dims=c(150,10)
coa1$l1  #Row scores dims =c(4989,10)
coa1$c1  #Col scores dims=c(150,10)
```

```{r}
sapply(list(li=coa1$li,  l1=coa1$l1, co=coa1$co, c1=coa1$c1), dim)
```

Results of ade4 matrix factorization can be interactively explored using explor, by default it plots COA results in the biplot, because the features (genes) and cases are on the same scale.  

The coordinates are in the object co and it can be seen that COA separates by cell cluster. explor creates  plots using scatterD3::scatterD3 which creates interactive scatter plot based on d3.js. 
```{r, eval=FALSE}
res <- explor::prepare_results(coa1)
explor::CA_var_plot(res, var_hide = "Row", col_var = pheno$clust)
```

```{r}
scatterD3::scatterD3(coa1$co[,1], coa1$co[,2], col_var= pheno$clust, ylab = "PC2", xlab="PC1")
```


```{r}
plot(coa1$co[,1], coa1$co[,2], col=pheno$clust, pch=19)
```
# glmPCA  
glmPCA is iterative, so in this example I will set seed so its reproducible. I discuss about the iterative nature of glmPCA below

```{r}
set.seed(50)
coa2<-glmpca::glmpca(mat,L=2,fam="poi")

dim(coa2$factors)  # Column scores dim=c(150,   2)
dim(coa2$loadings) # row scoress dim =c(4989 ,   2)
```

```{r}
plot(coa2$factors$dim1, coa2$factors$dim2, col=pheno$clust, pch=19)
```


# CORRAL
Correl from Lauren Hsu uses irbla and sparse matrix representation so is a fast implementation correspondence analysis in the function corral. Corral accepts input formats SingleCellExperiment (Bioconductor), SparseMatrix, matrix, data.frame. It doesn't yet support delayedArray.

```{r}
coa3<-corral::corral(mat,ncomp=2)
coa3
coa3$PCu[1:2,]  # Row coordinates  dims =c(4989,10)
coa3$PCv[1:2,]  # Col coordinates  dims=c(150,10)

plot(coa3$PCv[,1], coa3$PCv[,2], col=pheno$clust, pch=19)
```

 


# Comparing the output of all 3 COA approaches
Correlation between correspondence analysis and corral is identical

```{r}
cor(coa1$co[,1], coa3$PCv[,1])
cor(coa1$co[,2], coa3$PCv[,2])
```

```{r}
par(mfrow=c(1,2))
plot(coa1$co[,1], coa3$PCv[,1])
plot(coa1$co[,2], coa3$PCv[,2])
```

Correlation between glmPCA and corral. Note the PCs are flipped, PC1 in Corral shares r=0.99 correlation with PC2 of glmPCA(type = poi). 
```{r}
cor(coa2$factors[,1], coa3$PCv[,2])
cor(coa2$factors[,2], coa3$PCv[,1])
```
```{r}
par(mfrow=c(1,2))
plot(-coa2$factors[,1], coa3$PCv[,2])
plot(coa2$factors[,2], coa3$PCv[,1])
```




# Notes

GLM-PCA provides generalized PCA for probit and logit models but it is an iterative algorithm.  Between runs, it tends to flip the order of Prinipcal Components.

```{r}

iter=20
tt<-lapply(1:iter, function(x) glmpca::glmpca(mat, L=2, fam="poi",verbose=FALSE))
factors1<-sapply(1:iter, function(i) tt[[i]]$factors$dim1)  # first factor
factors2<-sapply(1:iter, function(i) tt[[i]]$factors$dim2)  # Second factor

colnames(factors1) = paste0("PC1", 1:iter, "_")
colnames(factors2) = paste0("PC2", 1:iter, "_")
```

```{r}
boxplot(factors1, ylab="factor PC1", xlab="iter", las=2, main="The First PC1 in 20 iterations")  # dim=c(150  , iter)
```

```{r}
ComplexHeatmap::Heatmap(cor(factors1), name="Correlations in PC1 (20 iterations)")
```



# References

Abdi, H., & Valentin, D. (2007). Multiple Correspondence Analysis. In N. Salkind (Ed.), Encyclopedia of Measurement and Statistics (pp. 652–657). Sage Publications, Inc. https://doi.org/10.4135/9781412952644.n299

Benzécri, J.-P. (Ed.). (1973). L’analyse des données. 

Busold, C. H., Winter, S., Hauser, N., Bauer, A., Dippon, J., Hoheisel, J. D., & Fellenberg, K. (2005). Integration of GO annotations in Correspondence Analysis: Facilitating the interpretation of microarray data. Bioinformatics, 21(10), 2424–2429. https://doi.org/10.1093/bioinformatics/bti367

Culhane, A C, Perrière, G., & Higgins, D. G. (2003). Cross-platform comparison and visualisation of gene expression data using co-inertia analysis. BMC Bioinformatics, 4(1), 59. https://doi.org/10.1186/1471-2105- 4-59

Culhane, A. C., Perriere, G., Considine, E. C., Cotter, T. G., & Higgins, D. G. (2002). Between-group analysis of microarray data. Bioinformatics, 18(12), 1600–1608. https://doi.org/10.1093/bioinformatics/18.12.1600

Duò, A., Robinson, M. D., & Soneson, C. (2018). A systematic performance evaluation of clustering methods for single cell RNA-seq data. F1000Research, 7, 1141. https://doi.org/10.12688/f1000research.15666.2

Fellenberg, K., Hauser, N. C., Brors, B., Neutzner, A., Hoheisel, J. D., & Vingron, M. (2001). Correspondence analysis applied to microarray data. Proceedings of the National Academy of Sciences, 98(19), 10781–10786. https://doi.org/10.1073/pnas.181597298

Grantham, R., Gautier, C., & Gouy, M. (1980). Codon frequencies in 119 individual genes confirm corsistent choices of degenerate bases according to genome type. Nucleic Acids Research, 8(9), 1893–1912. https://doi.org/10.1093/nar/8.9.1893

Greenacre, M. (2013). The contributions of rare objects in correspondence analysis. Ecology, 94(1), 241–249. https://doi.org/10.1890/11-1730.1

Greenacre, M. J. (1984). Theory and applications of correspondence analysis. Academic Press.

Greenacre, M. J. (2010). Correspondence analysis: Correspondence analysis. Wiley Interdisciplinary Reviews: Computational Statistics, 2(5), 613–619. https://doi.org/10.1002/wics.114

Greenacre, M., & Hastie, T. (1987). The Geometric Interpretation of Correspondence Analysis. Journal of the American Statistical Association, 82(398), 437–447.https://doi.org/10.1080/01621459.1987.10478446

Hafemeister, C., & Satija, R. (2019). Normalization and variance stabilization of single-cell RNA-seq data using regularized negative binomial regression. Genome Biology, 20(1), 296. https://doi.org/10.1186/s13059-019-1874-1

Hicks, S. C., Townes, F. W., Teng, M., & Irizarry, R. A. (2018). Missing data and technical variability in single-cell RNAsequencing experiments. Biostatistics, 19(4), 562–578. https://doi.org/10.1093/biostatistics/kxx053

Hirschfeld, H. O. (1935). A Connection between Correlation and Contingency. Mathematical Proceedings of the Cambridge Philosophical Society, 31(4), 520–524. https://doi.org/10.1017/S0305004100013517

Holmes, S. (2008). Multivariate data analysis: The French way. ArXiv:0805.2879 [Stat], 219–233. https://doi.org/10.1214/193940307000000455

Hsu L, Culhane A. Impact of Data Preprocessing on Integrative Matrix Factorization of Single Cell Data. Front Oncol. 2020;10:973. doi:10.3389/fonc.2020.00973.

Hubert, L., & Arabie, P. (1985). Comparing partitions. Journal of Classification, 2(1), 193–218. https://doi.org/10.1007/BF01908075

Legendre, P., Legendre, L., Legendre, L., & Legendre, L. (1998). Numerical ecology (2nd English ed). Elsevier.

Meng, C., Kuster, B., Culhane, A. C., & Gholami, A. (2014). A multivariate approach to the integration of multi-omics datasets. BMC Bioinformatics, 15(1), 162. https://doi.org/10.1186/1471-2105-15-162

Meng, C., Zeleznik, O. A., Thallinger, G. G., Kuster, B., Gholami, A. M., & Culhane, A. C. (2016). Dimension reduction techniques for the integrative analysis of multi-omics data. Briefings in Bioinformatics, 17(4), 628–641. https://doi.org/10.1093/bib/bbv108

Townes, F.W., Hicks, S.C., Aryee, M.J. et al. Feature selection and dimension reduction for single-cell RNA-Seq based on a multinomial model. Genome Biol 20, 295 (2019). https://doi.org/10.1186/s13059-019-1861-6 


